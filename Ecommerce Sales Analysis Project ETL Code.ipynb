{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "25b17620-931d-48cc-bac7-2423e816bbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sqlalchemy import create_engine\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9c29f41f-83b6-4af2-8f18-903320ab1584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " File 'customers_dataset.csv' has been loaded successfully with 99441 rows and 5 columns.\n",
      " File 'geolocation_dataset.csv' has been loaded successfully with 1000163 rows and 5 columns.\n",
      " File 'orders_dataset.csv' has been loaded successfully with 99441 rows and 8 columns.\n",
      " File 'order_items_dataset.csv' has been loaded successfully with 112650 rows and 7 columns.\n",
      " File 'order_payments_dataset.csv' has been loaded successfully with 103886 rows and 5 columns.\n",
      " File 'order_reviews_dataset.csv' has been loaded successfully with 99224 rows and 7 columns.\n",
      " File 'products_dataset.csv' has been loaded successfully with 32951 rows and 9 columns.\n",
      " File 'product_category_name_translation.csv' has been loaded successfully with 71 rows and 2 columns.\n",
      " File 'sellers_dataset.csv' has been loaded successfully with 3095 rows and 4 columns.\n",
      "Total number of CSV files loaded: 9\n",
      "All the files have been successfully loaded in the notebook\n",
      " The total time taken to load the datasets is 0.979565 seconds\n"
     ]
    }
   ],
   "source": [
    "## Defining a  scripting function to read all the files from the dataset\n",
    "def folder_files(folder):\n",
    "    folder_path = os.path.expanduser(folder)\n",
    "    csv_files = [file for file in os.listdir(folder_path) if file.endswith('.csv')]\n",
    "    \n",
    "    dataframes = []\n",
    "    for file in csv_files:\n",
    "        csv_path = os.path.join(folder_path, file)\n",
    "        df = pd.read_csv(csv_path)\n",
    "        dataframes.append(df)\n",
    "        print(f\" File '{file}' has been loaded successfully with {df.shape[0]} rows and {df.shape[1]} columns.\")\n",
    "    \n",
    "    print(f\"Total number of CSV files loaded: {len(dataframes)}\")\n",
    "    return dataframes\n",
    "\n",
    "ecommerce_folder_files = folder_files(r'C:\\Users\\Lenovo\\OneDrive\\Desktop\\Ecommerce Project')\n",
    "\n",
    "start_time= datetime.now()\n",
    "\n",
    "try:\n",
    "    customers_dataset= ecommerce_folder_files[0]\n",
    "    geolocation_dataset= ecommerce_folder_files[1]\n",
    "    orders_dataset= ecommerce_folder_files[2]\n",
    "    order_items_dataset= ecommerce_folder_files[3]\n",
    "    order_payments_dataset= ecommerce_folder_files[4]\n",
    "    order_reviews_dataset= ecommerce_folder_files[5]\n",
    "    products_dataset= ecommerce_folder_files[6]\n",
    "    product_category_name_translation= ecommerce_folder_files[7]\n",
    "    sellers_dataset= ecommerce_folder_files[8]\n",
    "    print(\"All the files have been successfully loaded in the notebook\")\n",
    "\n",
    "except IndexError as e_1:\n",
    "    print(\"Error: Some files are missing. Check if all required CSVs are present in the folder.\")\n",
    "    print(f\"Details: {e_1}\")\n",
    "\n",
    "except Exception as e_2:\n",
    "    print(\"An unexpected error occurred while loading the datasets.\")\n",
    "    print(f\"Details: {e_2}\")\n",
    "\n",
    "end_time= datetime.now()\n",
    "total_time_taken= abs((start_time-end_time)).total_seconds()\n",
    "print(f\" The total time taken to load the datasets is {total_time_taken} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "103a05cb-2f7d-4ad6-8b36-c5227218cf8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Total number of records in the customers dataset are 99441\n",
      " The total number of customer ids in the customers dataset are 99441\n",
      " The total number of unique customer ids in the customers dataset are 96096\n",
      " The total number of duplicates in the customers dataset are 0\n",
      " The primary key for the customers dataset is the column: customer_id whereas the foreign key is customer_unique_id\n"
     ]
    }
   ],
   "source": [
    "### Scripting For Data Cleaning and Data Pre-processing\n",
    "\n",
    "############################ Cleaning Customers Dataset ############################################################################\n",
    "def data_cleaning_customers(df):\n",
    "    df['customer_city']= df['customer_city'].str.title()\n",
    "    df[df.columns]= df.apply(lambda col: col.str.strip() if col.dtype == 'object' else col)\n",
    "    duplicate_records_customers = df[df.duplicated(subset=['customer_id'], keep=False)]\n",
    "    df = df.drop_duplicates(subset=['customer_id'], keep='first')\n",
    "    print(f\" Total number of records in the customers dataset are {df.shape[0]}\")\n",
    "    print(f\" The total number of customer ids in the customers dataset are {df['customer_id'].nunique()}\")\n",
    "    print(f\" The total number of unique customer ids in the customers dataset are {df['customer_unique_id'].nunique()}\")\n",
    "    print(f\" The total number of duplicates in the customers dataset are {df[df.duplicated()].size}\")\n",
    "    print(f\" The primary key for the customers dataset is the column: {df.columns[0]} whereas the foreign key is {df.columns[1]}\")\n",
    "    return df,duplicate_records_customers\n",
    "\n",
    "customers_dataset, duplicate_customers= data_cleaning_customers(customers_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c95e8112-e696-4586-94a1-5f0567a3fc85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The total number of records in the orders_dataset are 99441\n",
      " The total number of duplicated records in the orders_dataset are 0\n",
      " The total number of unique order ids in the orders_dataset are 99441\n",
      " The total number of unique customer ids in the orders_dataset are 99441\n",
      " For the orders_dataset, the column order_id serves as the primary key whereas the customer id column serves as the foreign key\n"
     ]
    }
   ],
   "source": [
    "\n",
    "########################### Cleaning the Orders Dataset #######################################################################\n",
    "def data_cleaning_orders(df1):\n",
    "    df1[df1.columns]= df1.apply(lambda col: col.str.strip() if col.dtype == 'object' else col)\n",
    "    df1['order_purchase_timestamp']= pd.to_datetime(df1['order_purchase_timestamp'].str[0:10],format=\"%d-%m-%Y\",dayfirst=True)\n",
    "    df1['order_approved_at']= pd.to_datetime(df1['order_approved_at'].str[0:10],format=\"%d-%m-%Y\",dayfirst=True)\n",
    "    df1['order_delivered_carrier_date']= pd.to_datetime(df1['order_delivered_carrier_date'].str[0:10],format=\"%d-%m-%Y\",dayfirst=True)\n",
    "    df1['order_delivered_customer_date']= pd.to_datetime(df1['order_delivered_customer_date'].str[0:10],format=\"%d-%m-%Y\",dayfirst=True)\n",
    "    df1['order_estimated_delivery_date']= pd.to_datetime(df1['order_estimated_delivery_date'].str[0:10],format=\"%d-%m-%Y\",dayfirst=True)\n",
    "    df1['order_status']= df1['order_status'].str.title()\n",
    "    duplicated_records_orders= df1[df1.duplicated(subset=['order_id','customer_id'], keep=False)]\n",
    "    print(f\" The total number of records in the orders_dataset are {df1.shape[0]}\")\n",
    "    print(f\" The total number of duplicated records in the orders_dataset are {duplicated_records_orders.shape[0]}\")\n",
    "    print(f\" The total number of unique order ids in the orders_dataset are {df1['order_id'].nunique()}\")\n",
    "    print(f\" The total number of unique customer ids in the orders_dataset are {df1['customer_id'].nunique()}\")\n",
    "    print(\" For the orders_dataset, the column order_id serves as the primary key whereas the customer id column serves as the foreign key\")\n",
    "    return df1,duplicated_records_orders\n",
    "\n",
    "orders_dataset, duplicate_orders = data_cleaning_orders(orders_dataset)\n",
    "orders_dataset.head(5)\n",
    "\n",
    "#############################################################################################################################################\n",
    "## The order_items_dataset contaisn the information about orders i.e. which products were included in that particular order, what was the shipping date \n",
    "## of the order, the order value as well as the freight value that is used in the transportation of the order. Hence we shall make aggregated table\n",
    "## from this particular dataset and then join with the order_dataset to fidn the values corresponding to each order and then make an aggregated table\n",
    "## for orders.\n",
    "\n",
    "def data_cleaning_order_items_dataset(df2):\n",
    "    df2['shipping_limit_date']= pd.to_datetime(df2['shipping_limit_date'].str[0:10],dayfirst=True)\n",
    "    df2[df2.columns]= df2.apply(lambda col: col.str.strip() if col.dtype== 'object' else col)\n",
    "    final_order_items_dataset= df2.groupby(['order_id']).agg(total_order_value=('price','sum'),total_freight_charges=('freight_value','sum')).reset_index().drop_duplicates()\n",
    "    return final_order_items_dataset\n",
    "\n",
    "aggregated_order_value= data_cleaning_order_items_dataset(order_items_dataset)\n",
    "aggregated_order_value\n",
    "\n",
    "##################################################################################################################################################\n",
    "## We will be performing join between the orders_dataset and the aggregated_order_value dataset to fetch the order value and freight cost for each order\n",
    "df3 = pd.merge(orders_dataset,aggregated_order_value, how='left',on='order_id')\n",
    "df3['total_order_value']= np.where((df3['total_order_value'].isnull()) & (df3['order_status'].isin(['Unavailable','Canceled'])),0,df3['total_order_value'])\n",
    "\n",
    "final_orders= df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f1729542-933b-4e02-a4ec-644703e8fe49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of duplicated records in the dataset are 0\n",
      "The total number of records after dropping the duplicates is 99224\n"
     ]
    }
   ],
   "source": [
    "\n",
    "########################### Cleaning the Order Reviews Dataset #######################################################################\n",
    "\n",
    "def orders_reviews_cleaning(df4):\n",
    "    df4[df4.columns]= df4.apply(lambda col: col.str.strip() if col.dtype == 'object' else col)\n",
    "    df4['review_creation_date']= pd.to_datetime(df4['review_creation_date'].str[0:10],format=\"%d-%m-%Y\",dayfirst=True)\n",
    "    df4['review_answer_timestamp']= pd.to_datetime( df4['review_answer_timestamp'].str[0:10],format=\"%d-%m-%Y\",dayfirst=True)\n",
    "    df4.drop(columns={'Column1','Column2'},inplace=True)\n",
    "    duplicated_records_orders_reviews= df4[df4.duplicated(subset=['review_id','order_id'],keep=False)]\n",
    "    df4= df4.drop_duplicates(subset=['review_id','order_id'], keep='first')\n",
    "    print(f\"The total number of duplicated records in the dataset are {duplicated_records_orders_reviews.shape[0]}\")\n",
    "    print(f\"The total number of records after dropping the duplicates is {df4.shape[0]}\")\n",
    "    return df4, duplicated_records_orders_reviews\n",
    "\n",
    "order_reviews_dataset, order_review_duplicated_records= orders_reviews_cleaning(order_reviews_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0c464f7f-e5a7-4ccc-9229-3d728bedf0f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>product_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1e9e8ef04dbcff4541ed26657ea517e5</td>\n",
       "      <td>Perfumery</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3aa071139cb16b67ca9e5dea641aaa2f</td>\n",
       "      <td>Art</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>96bd76ec8810374ed1b65e291975717f</td>\n",
       "      <td>Sports_Leisure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cef67bcfe19066a932b7673e239eb23d</td>\n",
       "      <td>Baby</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9dc1a7de274444849c219cff195d0b71</td>\n",
       "      <td>Housewares</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32946</th>\n",
       "      <td>a0b7d5a992ccda646f2d34e418fff5a0</td>\n",
       "      <td>Furniture_Decor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32947</th>\n",
       "      <td>bf4538d88321d0fd4412a93c974510e6</td>\n",
       "      <td>Construction_Tools_Lights</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32948</th>\n",
       "      <td>9a7c6041fa9592d9d9ef6cfe62a71f8c</td>\n",
       "      <td>Bed_Bath_Table</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32949</th>\n",
       "      <td>83808703fc0706a22e264b9d75f04a2e</td>\n",
       "      <td>Computers_Accessories</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32950</th>\n",
       "      <td>106392145fca363410d287a815be6de4</td>\n",
       "      <td>Bed_Bath_Table</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>32951 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             product_id           product_category\n",
       "0      1e9e8ef04dbcff4541ed26657ea517e5                  Perfumery\n",
       "1      3aa071139cb16b67ca9e5dea641aaa2f                        Art\n",
       "2      96bd76ec8810374ed1b65e291975717f             Sports_Leisure\n",
       "3      cef67bcfe19066a932b7673e239eb23d                       Baby\n",
       "4      9dc1a7de274444849c219cff195d0b71                 Housewares\n",
       "...                                 ...                        ...\n",
       "32946  a0b7d5a992ccda646f2d34e418fff5a0            Furniture_Decor\n",
       "32947  bf4538d88321d0fd4412a93c974510e6  Construction_Tools_Lights\n",
       "32948  9a7c6041fa9592d9d9ef6cfe62a71f8c             Bed_Bath_Table\n",
       "32949  83808703fc0706a22e264b9d75f04a2e      Computers_Accessories\n",
       "32950  106392145fca363410d287a815be6de4             Bed_Bath_Table\n",
       "\n",
       "[32951 rows x 2 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "########################### Cleaning the Orders Dataset #######################################################################\n",
    "## We shall be combining two datasets:products_dataset and product_category_name_translation to fetch the product names and the product category transalation\n",
    "\n",
    "def merging_product_info(df5,df6):\n",
    "    merged_products_info= pd.merge(df5[['product_id','product_category_name']],df6,how='left',on='product_category_name').drop_duplicates().reset_index(drop=True)\n",
    "    merged_products_info.drop(columns={'product_category_name'},inplace=True)\n",
    "    merged_products_info.rename(columns={'product_category_name_english':'product_category'},inplace=True)\n",
    "    merged_products_info['product_category']=merged_products_info['product_category'].str.strip().str.title()\n",
    "    merged_products_info['product_id']=merged_products_info['product_id'].str.strip()\n",
    "    return merged_products_info\n",
    "\n",
    "products_information= merging_product_info(products_dataset,product_category_name_translation)\n",
    "products_information['product_category']= products_information['product_category'].fillna('Catgegory_Unavailable')\n",
    "products_information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0893ca3d-82c3-42f6-8f20-20118eee63c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of duplicated records in the dataset are 0\n",
      "The total number of records after dropping the duplicates is 3095\n"
     ]
    }
   ],
   "source": [
    "########################### Cleaning the Sellers Information Dataset #######################################################################\n",
    "def sellers_dataset_cleaning(df7):\n",
    "    df7[df7.columns]= df7.apply(lambda col: col.str.strip() if col.dtype == 'object' else col)\n",
    "    df7['seller_city']= df7['seller_city'].str.strip().str.title()\n",
    "    duplicated_sellers_dataset= df7[df7.duplicated(subset=['seller_id'],keep=False)]\n",
    "    df7= df7.drop_duplicates(subset=['seller_id'], keep='first')\n",
    "    print(f\"The total number of duplicated records in the dataset are {duplicated_sellers_dataset.shape[0]}\")\n",
    "    print(f\"The total number of records after dropping the duplicates is {df7.shape[0]}\")\n",
    "    return df7, duplicated_sellers_dataset\n",
    "\n",
    "sellers_dataset, duplicated_sellers_records= sellers_dataset_cleaning(sellers_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "145a8176-8a4e-4c31-90df-6d0f900c48c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The connection string is successfully established\n"
     ]
    }
   ],
   "source": [
    "### EXPORTING THE DATASET TO THE SNOWFLAKE DATABASE FOR ANALYSIS\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "user = \"\"\n",
    "password = \"\"\n",
    "account = \"\"\n",
    "warehouse = \"\"\n",
    "database = \"\"\n",
    "schema = \"\"\n",
    "role = \"\"   # optional, remove from string if not needed\n",
    "\n",
    "try:\n",
    "    engine = create_engine(f'snowflake://{user}:{password}@{account}/{database}/{schema}?warehouse={warehouse}&role={role}')\n",
    "    print(\"The connection string is successfully established\")\n",
    "\n",
    "except Exception as e1:\n",
    "    print(\"The connection wasn't established, please check your credentials again\")\n",
    "    print(\"Error details:\", e1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "309597fb-a321-4e06-83a5-9964a02784c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3095"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Pushing the cleaned datasets to the snowflake databse\n",
    "customers_dataset.to_sql(\"customers_info\",con=engine,if_exists='replace',index=False, chunksize=10000)\n",
    "final_orders.to_sql(\"orders_info\",con=engine, if_exists='replace',index=False, chunksize=10000)\n",
    "order_items_dataset.to_sql(\"order_items_info\",con=engine, if_exists='replace',index=False, chunksize=10000)\n",
    "order_reviews_dataset.to_sql(\"order_reviews_info\",con=engine, if_exists='replace',index=False,chunksize=10000)\n",
    "products_information.to_sql(\"products_info\",con=engine, if_exists='replace',index=False,chunksize=10000)\n",
    "sellers_dataset.to_sql(\"sellers_info\",con=engine, if_exists='replace',index=False,chunksize=10000)###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "77b034d5-72ac-4d38-adef-c623a2204f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "############### EXPLORATORY DATA ANALYSIS #############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e4f29133-103e-44ce-9394-03136e08191e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112650\n",
      "order_id      98666\n",
      "product_id    32951\n",
      "seller_id      3095\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(order_items_dataset.shape[0])\n",
    "print(order_items_dataset[['order_id','product_id','seller_id']].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1ea2ee-6244-4f5a-8ac6-63b288e02e73",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
